from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np

from reverse_engineering.pomdp import POMDP

from .plot import plot
from .utils import create_maze, qd_matrix

size_x = 99  # length of maze (should be odd)
size_y = 19  # width of maze (should be odd)
size_view = 11  # size of agent-centered view (should be odd)
size_view_half = (size_view - 1) // 2

directions = np.array([[-1, 0], [1, 0], [0, -1], [0, 1]])  # (dy, dx) for up, down, left, right
n_decisions = len(directions)  # number of possible decisions per step
n_decision_steps = 4  # How many steps ahead to make decisions

T = 20000  # maximum duration
No = size_view * size_view  # dimensionality of observations
Ns = No  # dimensionality of hidden states
Nd = n_decisions**n_decision_steps  # dimensionality of decisions
sim_type = 2

risk_chunk_size = 200
delta_thresholds = [-10, 0, 10]
risk_values = [0.55, 0.55, 0.45, 0]

E_right = 0.25  # prior of selecting rightward motion
# In the paper 'Canonical neural networks perform active inference',
# E_right = 0.15 corresponds to the E_right = 0.0023 condition (black)
# E_right = 0.25 corresponds to the E_right = 0.0039 condition (blue)
# E_right = 0.35 corresponds to the E_right = 0.0055 condition (cyan)
# Please refer to Figs. 4 and 5 in the paper.


def create_init_pomdp(buf_size: int = T) -> POMDP:
    qa0 = np.ones((No * 2, Ns * 2), dtype=np.float32) * np.float32(10000000)
    qbi0 = np.ones((Ns * 2, Ns * 2), dtype=np.float32) * np.float32(10000000)
    qci0 = np.ones((Ns * 2, Nd * 2), dtype=np.float32) * np.float32(3000)
    qa0[:No, :Ns] += np.eye(No, Ns, dtype=np.float32) * np.float32(990000000)
    qa0[No:, Ns:] += np.eye(No, Ns, dtype=np.float32) * np.float32(990000000)
    D = np.ones((Ns * 2,), dtype=np.float32) * 0.5 / Nd
    E = np.ones((Nd * 2,), dtype=np.float32) * 0.5 / Nd

    E_up = 0.25
    E_down = 0.25
    E_left = 0.5 - E_right
    E[:Nd] = np.repeat([E_up, E_down, E_left, E_right], Nd // n_decisions) / (Nd // n_decisions)
    E[Nd:] = 1 - E[:Nd]

    pomdp = POMDP(qa0, D, qbi0, qci0, E, sim_type, buf_size=buf_size)

    return pomdp


def main(
    out_dir: Path | str | None = None,
    n_sample: int = 30,
    n_trial: int = 100,
    n_intvl: int = 2000,
) -> None:
    out_dir = Path(out_dir) if out_dir is not None else Path(__file__).parent / "output"
    out_dir.mkdir(parents=True, exist_ok=True)

    maze_list = [create_maze(size_x, size_y) for _ in range(20)]
    E_est_list = np.zeros((n_sample, n_trial, Nd), dtype=np.float32)
    fig = plt.figure(figsize=(17, 6))

    for idx_samp in range(n_sample):
        pomdp = create_init_pomdp(buf_size=T)
        maze = maze_list[idx_samp]
        time_history = np.zeros(n_trial, dtype=np.int32)

        for idx_trial in range(n_trial):

            def _plot(pos, o_mat, qs_mat, qd_mat, next_decision_prob):
                qd_mat = np.minimum(qd_mat * 32, 1)
                times = time_history[:idx_trial]
                plot(fig, maze, pos, o_mat, qs_mat, qd_mat, next_decision_prob, times, T, n_trial, n_intvl)
                plt.draw()
                plt.pause(0.001)

            pomdp, trial_time, F, E_est, pos = run_trial(
                pomdp,
                maze,
                T,
                n_intvl=n_intvl,
                plot_fn=_plot,
            )
            time_history[idx_trial] = trial_time + 1  # 1-indexed
            E_est_list[idx_samp, idx_trial] = E_est

            # logging and visualization
            if idx_samp == 0 and (idx_trial == 0 or idx_trial == n_trial - 1):
                dpi = fig.dpi
                fig.savefig(
                    out_dir / f"maze_sample{idx_samp + 1}_trial{idx_trial + 1}_E{E_right}.png",
                    format="png",
                    dpi=120,
                )
                fig.dpi = dpi  # reset dpi

    np.save(out_dir / f"Eest_E{E_right}.npy", E_est_list)


def run_trial(
    pomdp: POMDP,
    maze: np.ndarray,
    T: int | None = 10000,
    n_intvl: int | None = 2000,
    plot_fn=None,
) -> tuple[POMDP, int, float, np.ndarray, np.ndarray]:
    # position (y, x) of the agent on the maze; 0-indexed
    pos = np.zeros((2, T), dtype=np.int32)
    pos[:, 0] = [(size_y + 1) // 2 - 1, 1]  # start position: left top corner
    goal_pos = [(size_y + 1) // 2 - 1, size_x - 1]

    action: int | None = None  # action {0, 1, 2, 3} for up, down, left, right

    pomdp.reset_buffer()
    step_risks = np.zeros(T, dtype=np.float32)

    for t in range(1, T):
        # generative process
        y, x = pos[:, t - 1]

        if action is not None:
            dy, dx = directions[action]
            # Move 2 steps in the direction of the action if the path is clear
            # In a maze generated by create_maze.py,
            # if the agent is in a odd coordinate, passages are guaranteed to be at least 2 cells long
            # so checking the first cell (y + dy, x + dx) is sufficient to see if the path is clear
            if maze[y + dy, x + dx] == 0:
                y, x = y + dy * 2, x + dx * 2

        pos[:, t] = y, min(x, size_x - 1)

        y_min = max(y - size_view_half, 0)
        y_max = min(y + size_view_half, size_y - 1)
        x_min = max(x - size_view_half, 0)
        x_max = min(x + size_view_half, size_x - 1)
        view = maze[y_min : y_max + 1, x_min : x_max + 1]  # agent-centered view

        y_offset = max(size_view_half - y, 0)  # y-offset of the view within the obs matrix
        x_offset = max(size_view_half - x, 0)  # x-offset of the view within the obs matrix
        o_mat = np.ones((size_view, size_view), dtype=np.int32)
        o_mat[y_offset : y_offset + view.shape[0], x_offset : x_offset + view.shape[1]] = view

        s = o_mat.ravel(order="F")  # states (Ns,)
        # observations (No * 2,)
        # o[:, :No] contains bits representing whether o^(0), ..., o^(No-1) is 1.
        # o[:, No:] contains bits representing whether o^(0), ..., o^(No-1) is 0.
        o = np.concatenate([s, 1 - s])

        # inference
        qs, qd_prob, d = pomdp.infer(o)
        action = np.argmax(d) // (Nd // n_decisions)  # in {0, 1, 2, 3}

        # risk
        if pos[1, t] > pos[1, t - 1]:
            risk = 0.0
        elif pos[1, t] < pos[1, t - 1]:
            risk = 1.0
        else:
            risk = 0.5
        step_risks[t] = risk

        # judge whether the agent reaches the goal
        done = (pos[:, t] == goal_pos).all()

        # figure output
        if plot_fn is not None and ((t + 1) % n_intvl == 0 or done):
            qs_mat = np.reshape(qs[:Ns], (size_view, size_view), order="F")
            qd_mat = qd_matrix(qd_prob, directions, n_decision_steps, size_view)
            qd_prob_next = qd_prob.reshape((n_decisions, Nd // n_decisions)).sum(axis=1)  # (n_decisions, )
            plot_fn(pos[:, : t + 1], o_mat, qs_mat, qd_mat, qd_prob_next)
            plt.pause(0.001)

        if done:
            break

    # compute risk
    chunk_risks = compute_risk(step_risks[: t + 1], risk_chunk_size, delta_thresholds, risk_values)

    # compute variational free energy
    F = pomdp.compute_vfe(chunk_risks, risk_chunk_size)

    # estimate E based exclusively on decisions
    E_est = pomdp.d_buf[:, 2 : pomdp.buf_idx].mean(axis=-1)

    # update
    pomdp.learn(chunk_risks, risk_chunk_size)

    return pomdp, t, F, E_est, pos


def compute_risk(
    step_risks: np.ndarray,
    chunk_size: int,
    delta_thresholds: list[float],
    risk_values: list[float],
) -> np.ndarray:
    chunk_risks = []
    for i in range(0, len(step_risks) - 1, chunk_size):
        chunk_mean_risk = step_risks[i : i + chunk_size].mean()  # in [0, 1]
        delta = (1 - chunk_mean_risk * 2) * chunk_size  # in [-chunk_size, chunk_size]
        if delta <= delta_thresholds[0]:
            risk = risk_values[0]
        elif delta <= delta_thresholds[1]:
            risk = risk_values[1]
        elif delta <= delta_thresholds[2]:
            risk = risk_values[2]
        else:
            risk = risk_values[3]
        chunk_risks.append(risk)

    return np.array(chunk_risks, dtype=np.float32)


if __name__ == "__main__":
    np.random.seed(9)
    main(n_sample=1)
